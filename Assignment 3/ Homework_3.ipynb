{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework 3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOUSY7znXPUUWHURrMceB+2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayuri-lalwani/Deep-Learning/blob/main/Assignment%203/%20Homework_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRyhBcCy8l6P"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFqTljUsMhku"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCZoSXgJj3Ln"
      },
      "source": [
        "Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvUk1HQQ6-me",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9500c3b0-a816-4dd9-b7f3-5981169281d2"
      },
      "source": [
        "# 3 Dimension non-linear input data\n",
        "\n",
        "n = 200\n",
        "d = 3\n",
        "x = np.random.uniform(-1, 1, (n, d))\n",
        "\n",
        "weights_true = np.array([[2, 1,-1],[3,0,-1],]).T\n",
        "bias_true = np.array([0.5,0.1])\n",
        "\n",
        "y_true = (x ** 2) @ weights_true + x @ weights_true + bias_true\n",
        "print(f'x: {x.shape}, weights: {weights_true.shape}, bias: {bias_true.shape}, y: {y_true.shape}')\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: (200, 3), weights: (3, 2), bias: (2,), y: (200, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmtkCAL2Zu5V"
      },
      "source": [
        "# Let's use gradient descent to learn the weights and bias that minimizes the loss function.\n",
        "# For this, we need the gradient of the loss function and the gradients of the linear function.\n",
        "\n",
        "class MSE:\n",
        "  def __call__(self, y_pred, y_true):\n",
        "    self.y_pred = y_pred\n",
        "    self.y_true = y_true\n",
        "    return ((y_pred - y_true) ** 2).mean()\n",
        "\n",
        "  def backward(self):\n",
        "    n = self.y_true.shape[0]\n",
        "    self.gradient = 2. * (self.y_pred - self.y_true) / n\n",
        "    # print('MSE backward', self.y_pred.shape, self.y_true.shape, self.gradient.shape)\n",
        "    return self.gradient\n",
        "\n",
        "\n",
        "class Linear:\n",
        "  def __init__(self, input_dim: int, num_hidden: int = 1):\n",
        "    self.weights = np.random.randn(input_dim, num_hidden) * np.sqrt(2. / input_dim)\n",
        "    self.bias = np.zeros(num_hidden)\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    self.x = x\n",
        "    output = x @ self.weights + self.bias\n",
        "    #output = (x ** 2) @ weights_true + x @ weights_true + bias_true\n",
        "    return output\n",
        "\n",
        "  def backward(self, gradient):\n",
        "    self.weights_gradient = self.x.T @ gradient\n",
        "    self.bias_gradient = gradient.sum(axis=0)\n",
        "    self.x_gradient = gradient @ self.weights.T\n",
        "    return self.x_gradient\n",
        "\n",
        "  def update(self, lr):\n",
        "    self.weights = self.weights - lr * self.weights_gradient\n",
        "    self.bias = self.bias - lr * self.bias_gradient"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKRdCNp-kJLf"
      },
      "source": [
        "Adding non-linearity: Relu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJPRtPH6Spsu",
        "outputId": "9b2f557b-0e4b-43f2-b807-94b2e7502d38"
      },
      "source": [
        "# In order to learn non-linear functions, we need non-linearities in our model.\n",
        "\n",
        "class Relu:\n",
        "    def __call__(self, input_):\n",
        "        self.input_ = input_\n",
        "        self.output = np.clip(self.input_, 0, None)\n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, output_gradient):\n",
        "      # import pdb; pdb.set_trace()  # By the way, this is how you can debug\n",
        "      self.input_gradient = (self.input_ > 0) * output_gradient\n",
        "      return self.input_gradient\n",
        "\n",
        "\n",
        "relu = Relu()\n",
        "input_ = np.expand_dims(np.array([1, 0.5, 0, -0.5, -1]), -1)\n",
        "print(relu(input_))\n",
        "print(relu.backward(input_))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. ]\n",
            " [0.5]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]]\n",
            "[[ 1. ]\n",
            " [ 0.5]\n",
            " [ 0. ]\n",
            " [-0. ]\n",
            " [-0. ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4FX-akdStFn"
      },
      "source": [
        "Train our new non linear model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-bAkxcxSxbL",
        "outputId": "63969473-efc6-4973-b225-f594bf8a5ef0"
      },
      "source": [
        "class Model:\n",
        "  def __init__(self, input_dim, num_hidden, output_dim):\n",
        "    self.linear1 = Linear(input_dim, num_hidden)\n",
        "    self.relu1 = Relu()\n",
        "    self.relu2 = Relu()\n",
        "    self.linear2 = Linear(num_hidden, output_dim)\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    l1 = self.linear1(x)\n",
        "    r1 = self.relu1(l1)\n",
        "    r2 = self.relu2(r1)\n",
        "    l2 = self.linear2(r2)\n",
        "    return l2\n",
        "  \n",
        "  def backward(self, output_gradient):\n",
        "    linear2_gradient = self.linear2.backward(output_gradient)\n",
        "    relu2_gradient = self.relu2.backward(linear2_gradient)\n",
        "    relu1_gradient = self.relu1.backward(relu2_gradient)\n",
        "    linear1_gradient = self.linear1.backward(relu1_gradient)\n",
        "    # print('Model backward', linear2_gradient.shape, relu_gradient.shape, linear1_gradient.shape)\n",
        "    # import pdb; pdb.set_trace()\n",
        "    return linear1_gradient\n",
        "\n",
        "  def update(self, lr):\n",
        "    self.linear2.update(lr)\n",
        "    self.linear1.update(lr)\n",
        "\n",
        "loss = MSE()\n",
        "model = Model(d, 10,output_dim=2)\n",
        "y_pred = model(x)\n",
        "loss_value = loss(y_pred, y_true)\n",
        "loss_gradient = loss.backward()\n",
        "print(loss_value)\n",
        "model.backward(loss_gradient)\n",
        "#plot_3d(x, y_true, y_pred)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.465571848314308\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 7.44818433e-03, -4.14124519e-03,  5.45782577e-03],\n",
              "       [-4.43299380e-04, -1.92181334e-03,  2.19222544e-03],\n",
              "       [ 1.32073014e-02,  2.31484808e-03, -5.12966698e-03],\n",
              "       [ 2.29105456e-02,  1.41184995e-02, -1.01177182e-02],\n",
              "       [ 1.75787516e-02, -8.93761105e-03, -7.56682882e-03],\n",
              "       [ 1.52848809e-03, -4.39602663e-03,  4.13828194e-03],\n",
              "       [ 9.14760689e-03, -8.28661670e-03,  2.18992502e-03],\n",
              "       [ 9.22604974e-03, -1.11282836e-02, -1.10699120e-02],\n",
              "       [ 4.74483818e-03, -5.61318367e-03,  6.00900773e-03],\n",
              "       [-9.81402759e-03, -9.12874777e-03,  5.22259929e-03],\n",
              "       [-5.02541728e-03, -7.68882280e-03,  6.41634431e-03],\n",
              "       [ 6.06002590e-02, -2.83565769e-02, -2.05895726e-02],\n",
              "       [ 5.28948139e-03, -5.40175180e-03, -1.93839686e-02],\n",
              "       [ 3.80490308e-03,  9.12259642e-04, -1.08160155e-03],\n",
              "       [ 1.07482679e-02, -3.57848349e-02, -3.48729562e-02],\n",
              "       [-2.20578871e-02, -1.12353784e-02,  8.07155097e-03],\n",
              "       [ 6.13314524e-02, -2.83109484e-02, -1.99697428e-02],\n",
              "       [ 4.05101748e-03, -6.35303809e-03,  6.45864422e-03],\n",
              "       [ 5.24995882e-03, -1.01152986e-02, -1.04645342e-02],\n",
              "       [-1.41404886e-02, -6.29258021e-03,  7.72224394e-03],\n",
              "       [ 8.21582762e-03,  2.73807724e-03,  4.31671088e-03],\n",
              "       [ 5.68177570e-02, -2.92966530e-02, -2.53723866e-02],\n",
              "       [ 9.11439718e-04, -2.29157955e-03, -1.63875664e-02],\n",
              "       [-5.13934609e-03, -5.99594118e-04,  8.02881419e-03],\n",
              "       [ 3.29364902e-03,  1.74957393e-03,  3.05183340e-03],\n",
              "       [ 3.65157803e-02, -9.56884838e-03,  4.42664794e-03],\n",
              "       [-3.08249353e-03,  4.46590274e-04,  1.23569340e-03],\n",
              "       [ 6.41965600e-03, -9.29324990e-03,  9.57589450e-03],\n",
              "       [ 2.57322767e-02, -3.67024002e-03,  9.80238545e-03],\n",
              "       [ 3.35877725e-02, -3.11763080e-02, -1.70370235e-02],\n",
              "       [ 2.05920204e-03, -4.29834494e-03,  4.19288364e-03],\n",
              "       [ 5.67569931e-02, -2.14814613e-02, -7.91651980e-03],\n",
              "       [ 2.28262765e-03, -1.06430708e-03,  3.10241190e-03],\n",
              "       [-2.05324635e-03,  8.00278137e-04,  1.83722647e-03],\n",
              "       [ 1.61452398e-02, -3.58638038e-03,  3.40015129e-03],\n",
              "       [-5.98383937e-03, -3.39080706e-03,  1.45346819e-03],\n",
              "       [ 2.66724972e-03, -4.04504560e-03,  1.15434600e-03],\n",
              "       [ 5.97674962e-03, -1.26099898e-02, -1.38432437e-02],\n",
              "       [ 1.67053481e-02, -6.03903024e-03,  9.47568280e-03],\n",
              "       [ 1.91852391e-03, -2.06177483e-03, -1.77609523e-02],\n",
              "       [ 7.61729941e-03, -6.46617140e-03, -2.54558639e-02],\n",
              "       [ 7.51082612e-04, -4.21836404e-04,  7.88316756e-04],\n",
              "       [ 1.06401327e-02,  7.42110590e-03,  1.56493644e-02],\n",
              "       [ 7.07189087e-03, -1.63358734e-02, -1.88758593e-02],\n",
              "       [ 2.66151161e-02, -2.86753766e-02, -3.15285748e-02],\n",
              "       [ 1.94179262e-02, -4.03946890e-03,  8.47782260e-03],\n",
              "       [ 1.65568756e-03,  1.06424780e-03,  8.97027936e-03],\n",
              "       [ 7.10956709e-03, -2.59818666e-02, -3.58347945e-02],\n",
              "       [ 3.51776728e-02, -3.93941671e-02, -4.18486501e-02],\n",
              "       [ 5.47067900e-02, -1.38800374e-02,  7.65223671e-03],\n",
              "       [ 6.51750754e-03, -2.38690697e-02, -2.20240423e-02],\n",
              "       [-2.87689486e-03, -2.37629827e-03, -6.49697642e-03],\n",
              "       [-2.57038791e-02, -1.32054724e-02,  9.50653828e-03],\n",
              "       [-7.06115607e-04, -1.39198894e-03, -6.49473690e-03],\n",
              "       [ 2.62660524e-02, -1.10021629e-02, -6.03917880e-03],\n",
              "       [ 2.90601179e-02,  5.21853605e-03, -1.16289688e-02],\n",
              "       [ 7.01251483e-03, -1.40345598e-02, -1.89847606e-02],\n",
              "       [ 4.44268430e-02, -1.48823993e-02, -1.14807016e-02],\n",
              "       [ 1.35682969e-02, -1.50367919e-02, -3.18305146e-02],\n",
              "       [ 1.21720410e-02,  5.99311137e-03,  5.43963406e-03],\n",
              "       [ 8.47133493e-03, -5.17637243e-03,  1.19924632e-03],\n",
              "       [-5.91172294e-03,  2.02654839e-03,  1.44427308e-02],\n",
              "       [ 4.92295433e-03, -9.08483100e-03,  2.71698981e-03],\n",
              "       [ 3.91905270e-02, -4.45074400e-03, -2.46238766e-02],\n",
              "       [ 3.85366578e-02, -1.45575666e-02, -5.31279770e-03],\n",
              "       [-3.83923030e-03,  1.05653925e-03,  3.55806890e-03],\n",
              "       [ 3.21303450e-02,  5.56574234e-03, -1.22995674e-02],\n",
              "       [ 7.66678100e-03, -2.95594782e-05,  8.69504701e-03],\n",
              "       [-1.34717343e-02, -7.71558171e-03, -9.75065224e-03],\n",
              "       [ 1.45744195e-03, -1.12794036e-03,  4.34882723e-04],\n",
              "       [ 4.17456733e-02, -2.35200032e-02, -2.31085726e-02],\n",
              "       [ 3.07216884e-02, -1.61257659e-02, -1.43569042e-02],\n",
              "       [ 2.33707565e-03, -5.24237293e-03,  5.06849094e-03],\n",
              "       [-2.17137849e-02, -8.96702151e-03,  5.30210471e-03],\n",
              "       [ 5.03303710e-03, -1.06048087e-02,  1.03323157e-02],\n",
              "       [-1.75487825e-03,  1.02178775e-03,  8.66357492e-03],\n",
              "       [ 1.18831210e-03, -1.24563336e-03, -1.08106430e-02],\n",
              "       [ 1.81007545e-02, -1.90551228e-02, -1.67608836e-02],\n",
              "       [ 8.05664630e-03, -1.41626291e-02, -2.10179225e-02],\n",
              "       [ 6.45604860e-05,  1.21667652e-03, -4.48245143e-04],\n",
              "       [ 5.34147275e-03, -5.57007778e-03, -1.97903076e-02],\n",
              "       [ 4.24896501e-03, -7.57377670e-03,  7.54902353e-03],\n",
              "       [-2.13026497e-03, -2.46626056e-03,  1.64702986e-03],\n",
              "       [ 3.71113362e-03, -5.67213240e-03,  1.62205212e-03],\n",
              "       [ 1.37253127e-03, -5.77903207e-03, -6.80621801e-03],\n",
              "       [ 5.41450733e-02, -5.93605872e-03, -3.37633323e-02],\n",
              "       [ 8.69193955e-03, -4.69306393e-03, -2.40179041e-02],\n",
              "       [ 7.63400449e-04, -3.51136095e-04,  6.24886740e-06],\n",
              "       [-2.28686371e-03,  1.32255062e-03,  6.91056026e-03],\n",
              "       [-5.15534087e-03, -9.14563563e-03,  6.44608646e-03],\n",
              "       [ 5.61868940e-02, -2.13539709e-02, -1.93951972e-02],\n",
              "       [-9.24061707e-04, -2.93254547e-03,  3.74931352e-04],\n",
              "       [ 5.40520669e-03, -9.98413120e-03, -6.41964848e-03],\n",
              "       [ 1.63180998e-02,  9.37285125e-03, -6.92087389e-03],\n",
              "       [ 3.71234012e-03, -5.89866392e-03,  1.73015601e-03],\n",
              "       [ 6.99215423e-03,  1.82683813e-03,  3.92222696e-03],\n",
              "       [-6.17514075e-03, -1.15615286e-02,  8.23764079e-03],\n",
              "       [ 6.41690154e-03, -7.96462218e-03,  2.25018792e-03],\n",
              "       [ 1.33655773e-02, -5.66765979e-03,  8.29279662e-03],\n",
              "       [ 1.18775018e-02,  1.04035105e-02,  2.20644316e-02],\n",
              "       [ 1.42693608e-02, -1.11485098e-02, -3.15878298e-02],\n",
              "       [-4.29382679e-03, -2.20660982e-03, -3.98405603e-03],\n",
              "       [ 3.82277848e-03, -6.06819629e-03,  1.75250575e-03],\n",
              "       [ 3.54362165e-02, -3.82348300e-02, -4.19847239e-02],\n",
              "       [ 4.35663618e-02, -1.57479758e-02, -4.41734058e-03],\n",
              "       [ 2.51139890e-02, -2.18788510e-02, -2.89731681e-02],\n",
              "       [ 6.88630211e-03, -3.98188022e-03, -1.95224401e-02],\n",
              "       [ 5.59441248e-02, -3.04559778e-02, -2.85867208e-02],\n",
              "       [-1.27683958e-03,  3.07379759e-03,  1.36037272e-02],\n",
              "       [-6.78149639e-03, -1.98737921e-03,  6.27159011e-03],\n",
              "       [ 2.31232494e-04, -1.71653994e-03, -2.77280599e-03],\n",
              "       [-2.66874111e-04, -4.47782939e-03,  3.97255017e-03],\n",
              "       [-6.13518922e-03, -3.56843414e-03, -4.77177905e-03],\n",
              "       [-3.75622404e-03,  1.44889159e-03,  9.71658924e-03],\n",
              "       [ 4.25494088e-03, -7.49357463e-03,  7.48231233e-03],\n",
              "       [ 4.23612091e-03, -6.44002796e-03, -1.06438432e-02],\n",
              "       [ 1.00092161e-02, -2.21535395e-02, -2.50097316e-02],\n",
              "       [-4.17693995e-03, -4.32944152e-03,  1.78825080e-03],\n",
              "       [ 1.48141907e-02, -3.83638381e-02, -4.36318293e-02],\n",
              "       [-1.36541003e-02, -6.80922561e-03,  4.86643933e-03],\n",
              "       [ 4.67691157e-03, -1.71106090e-03,  8.35198117e-03],\n",
              "       [ 7.06592872e-02, -3.81674549e-02, -3.54354691e-02],\n",
              "       [ 5.53305763e-02, -2.30553196e-02, -1.24503859e-02],\n",
              "       [ 1.00930618e-03, -6.70134265e-04,  6.93905289e-04],\n",
              "       [ 8.93241956e-03,  5.50250995e-03, -3.94386593e-03],\n",
              "       [-9.48746812e-03, -7.21614304e-03, -1.76665764e-02],\n",
              "       [ 2.14775290e-02, -1.69049231e-02, -1.78875342e-02],\n",
              "       [-4.94951763e-03, -2.42312854e-03, -1.08867846e-03],\n",
              "       [ 1.37264731e-02, -5.02027285e-03,  7.83545456e-03],\n",
              "       [-6.41191220e-03, -8.74980424e-03,  5.78305356e-03],\n",
              "       [ 1.81019124e-02,  8.41348820e-03,  1.65692735e-02],\n",
              "       [-5.37655632e-03, -2.99693248e-03,  2.32881464e-03],\n",
              "       [ 6.71798215e-03, -9.75763727e-03,  2.82673431e-03],\n",
              "       [ 2.78105761e-05,  4.99939849e-03,  1.70259845e-02],\n",
              "       [ 3.27414579e-02, -2.92869821e-02, -3.77964379e-02],\n",
              "       [-9.17405202e-03, -7.42501724e-03,  3.93860648e-03],\n",
              "       [ 3.69852370e-02,  6.51419154e-03, -1.44517919e-02],\n",
              "       [ 5.38644645e-04,  1.02413705e-03,  7.60365435e-03],\n",
              "       [ 5.28540411e-03, -6.01801134e-03,  6.49386591e-03],\n",
              "       [ 1.14805094e-03, -1.20267562e-02, -2.45356875e-02],\n",
              "       [ 1.41682195e-02,  7.03740650e-03, -5.54906726e-03],\n",
              "       [ 2.88936395e-02, -2.86336502e-02, -3.34411951e-02],\n",
              "       [ 3.11970054e-03, -1.05127836e-02,  9.75740409e-03],\n",
              "       [-2.50295229e-03, -1.50885476e-03, -2.26816889e-03],\n",
              "       [-2.88511318e-03, -4.58434995e-03, -1.98528003e-02],\n",
              "       [ 1.78798396e-02, -2.18270274e-02, -1.62154572e-02],\n",
              "       [ 1.74332773e-02,  7.72090920e-03,  2.79105328e-03],\n",
              "       [ 1.30074960e-02, -1.65552163e-02, -1.57095807e-02],\n",
              "       [-1.77422251e-03,  3.23465485e-03,  1.53590128e-02],\n",
              "       [ 2.05876079e-02, -1.28063796e-02, -4.42480783e-02],\n",
              "       [ 4.76709022e-03, -5.66071332e-03, -1.13361376e-02],\n",
              "       [ 3.20463777e-03, -1.95545437e-02, -2.99849840e-02],\n",
              "       [ 1.79463596e-02, -1.60485637e-02, -4.05476735e-02],\n",
              "       [ 3.15360252e-03, -1.19845973e-03, -7.95440338e-05],\n",
              "       [ 1.55001679e-04,  9.65536989e-04,  6.82449027e-03],\n",
              "       [ 1.28561822e-02,  4.66085778e-04, -3.24829265e-02],\n",
              "       [-5.72031070e-03,  2.67791878e-03,  1.61144053e-02],\n",
              "       [ 5.10812726e-02, -2.76820439e-02, -2.58184038e-02],\n",
              "       [ 1.80037479e-02,  1.05605820e-02, -7.72754500e-03],\n",
              "       [ 2.06588066e-02,  3.57837350e-03, -7.90762287e-03],\n",
              "       [ 3.57684616e-03, -6.73072274e-03,  2.02116085e-03],\n",
              "       [ 2.56180651e-02, -6.16618253e-03, -2.11386436e-02],\n",
              "       [ 5.97483034e-02, -2.13265847e-02, -5.45197611e-03],\n",
              "       [ 5.55786093e-03,  1.27661645e-03,  3.18769490e-03],\n",
              "       [ 2.07181346e-03, -6.53920149e-03,  1.41072257e-02],\n",
              "       [ 2.92429759e-02, -1.09841246e-02, -3.89121329e-03],\n",
              "       [-1.24198731e-03, -6.92605039e-03,  5.57260661e-03],\n",
              "       [-4.46736179e-03, -4.72695572e-03, -9.04861209e-03],\n",
              "       [-1.01249952e-02,  2.41932516e-03,  6.84677738e-03],\n",
              "       [ 1.86284082e-03,  5.68888586e-03,  1.25800668e-02],\n",
              "       [-1.99309921e-03,  2.60381363e-03,  7.19901746e-03],\n",
              "       [ 4.60595065e-03, -1.56590707e-03, -2.21370055e-02],\n",
              "       [-3.82146596e-03, -3.60927498e-03,  1.63974726e-03],\n",
              "       [ 6.03397489e-03,  2.41473220e-03,  4.98556390e-03],\n",
              "       [ 3.57889768e-02, -7.33557774e-03, -2.76293813e-02],\n",
              "       [ 5.92293301e-03, -1.28340242e-02,  1.24606254e-02],\n",
              "       [ 3.50030323e-02, -1.12387473e-02, -3.83381358e-04],\n",
              "       [ 6.35040775e-03, -6.01801469e-03, -1.44851997e-02],\n",
              "       [ 8.48483047e-03, -1.16864350e-02, -2.08286882e-02],\n",
              "       [-3.22036571e-03, -1.70527574e-03, -3.08953217e-03],\n",
              "       [ 2.06776933e-03, -4.31450711e-03,  4.20886325e-03],\n",
              "       [-1.33402844e-03, -3.70397517e-03, -7.26101950e-03],\n",
              "       [-1.28886737e-03, -1.04559003e-02, -1.55707651e-02],\n",
              "       [ 1.38557634e-02, -5.34443804e-03,  8.14491638e-03],\n",
              "       [ 8.90102944e-04,  3.27596327e-04,  3.13226248e-03],\n",
              "       [ 7.16905707e-05, -4.05019521e-03,  3.62092404e-03],\n",
              "       [ 4.98538188e-02, -2.76780599e-02, -2.66784853e-02],\n",
              "       [ 4.87875144e-03,  1.84946866e-03, -1.67096022e-03],\n",
              "       [ 4.10765800e-02,  1.42667914e-02, -7.33112512e-03],\n",
              "       [ 2.17133843e-02, -4.17024889e-03,  9.18489521e-03],\n",
              "       [ 5.15093591e-03, -8.63508642e-03,  2.52750360e-03],\n",
              "       [ 5.14151344e-03, -9.61265938e-03, -9.73041115e-03],\n",
              "       [ 2.47289310e-02,  9.00115684e-03, -3.76190673e-03],\n",
              "       [ 2.59217947e-02, -3.35946584e-03,  9.58706445e-03],\n",
              "       [ 3.13970165e-02,  5.35973229e-03, -1.18029539e-02],\n",
              "       [ 3.82915602e-02, -5.05367803e-03, -2.49084979e-02],\n",
              "       [-6.83699825e-03, -3.43920529e-03, -2.06141631e-03],\n",
              "       [ 4.58681228e-02, -8.27534311e-04,  7.36119766e-03],\n",
              "       [-2.89748792e-03,  2.94693227e-03,  7.80861569e-03],\n",
              "       [-7.63566535e-03, -2.68660531e-03,  4.69189913e-03]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aP5dJnJfnGF"
      },
      "source": [
        "from typing import Callable\n",
        "\n",
        "def fit(x: np.ndarray, y: np.ndarray, model: Callable, loss: Callable, lr: float, num_epochs: int):\n",
        "  for epoch in range(num_epochs):\n",
        "    y_pred = model(x)\n",
        "    loss_value = loss(y_pred, y)\n",
        "    print(f'Epoch {epoch}, loss {loss_value}')\n",
        "    gradient_from_loss = loss.backward()\n",
        "    model.backward(gradient_from_loss)\n",
        "    model.update(lr)\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMjBrUWCktpj"
      },
      "source": [
        "Initializing non-linear and loss for three dimensional data<br/>\n",
        "3-d input and 2-d output with 2 hidden layers, 10 neurons for hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1l2HEUUOS6P4",
        "outputId": "3c7a1a8e-117c-40b6-855b-eecaceed9643"
      },
      "source": [
        "\n",
        "loss = MSE()\n",
        "nonlinear = Model(3,20,2)\n",
        "y_pred = nonlinear(x)\n",
        "print(x.shape, weights_true.shape, y_true.shape, y_pred.shape)\n",
        "print(loss(y_pred,y_true))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200, 3) (3, 2) (200, 2) (200, 2)\n",
            "5.118961022288858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXam6QFeS7Bm",
        "outputId": "17748ba5-83b1-40e0-941c-2c7e96f1ee81"
      },
      "source": [
        "fit(x, y_true, model=nonlinear, loss=loss, lr=0.1, num_epochs=400)\n",
        "y_pred = nonlinear(x)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss 0.10812060268520675\n",
            "Epoch 1, loss 0.1037469567658415\n",
            "Epoch 2, loss 0.09964683845657962\n",
            "Epoch 3, loss 0.09581019281681184\n",
            "Epoch 4, loss 0.09218438083417499\n",
            "Epoch 5, loss 0.08874491184508213\n",
            "Epoch 6, loss 0.08543219122424343\n",
            "Epoch 7, loss 0.08235346485267586\n",
            "Epoch 8, loss 0.07952291702503936\n",
            "Epoch 9, loss 0.07690210024246197\n",
            "Epoch 10, loss 0.07449454243178634\n",
            "Epoch 11, loss 0.07223129589624784\n",
            "Epoch 12, loss 0.07012572158026914\n",
            "Epoch 13, loss 0.0681492170662241\n",
            "Epoch 14, loss 0.0662843917331307\n",
            "Epoch 15, loss 0.06450545294747034\n",
            "Epoch 16, loss 0.06284004776197315\n",
            "Epoch 17, loss 0.06128352034268616\n",
            "Epoch 18, loss 0.059828595041792824\n",
            "Epoch 19, loss 0.058437018801955565\n",
            "Epoch 20, loss 0.05711878727854556\n",
            "Epoch 21, loss 0.05586255485042511\n",
            "Epoch 22, loss 0.05466219218072915\n",
            "Epoch 23, loss 0.053535347937308526\n",
            "Epoch 24, loss 0.052457854595245094\n",
            "Epoch 25, loss 0.05142257288619349\n",
            "Epoch 26, loss 0.05043243984446047\n",
            "Epoch 27, loss 0.04949764129178183\n",
            "Epoch 28, loss 0.04861685712108228\n",
            "Epoch 29, loss 0.04778561624045878\n",
            "Epoch 30, loss 0.04701744790872454\n",
            "Epoch 31, loss 0.04629441403432825\n",
            "Epoch 32, loss 0.04561437385939746\n",
            "Epoch 33, loss 0.04497010767129811\n",
            "Epoch 34, loss 0.04436342234801128\n",
            "Epoch 35, loss 0.043797871061271525\n",
            "Epoch 36, loss 0.043265848755579164\n",
            "Epoch 37, loss 0.042762471070460606\n",
            "Epoch 38, loss 0.0422840378743237\n",
            "Epoch 39, loss 0.0418286032932095\n",
            "Epoch 40, loss 0.04139918880969548\n",
            "Epoch 41, loss 0.0409894865527133\n",
            "Epoch 42, loss 0.04060299797791071\n",
            "Epoch 43, loss 0.04023377062939741\n",
            "Epoch 44, loss 0.03988345715380297\n",
            "Epoch 45, loss 0.03954468925659187\n",
            "Epoch 46, loss 0.03921374769885827\n",
            "Epoch 47, loss 0.03889842713934852\n",
            "Epoch 48, loss 0.03862488838812652\n",
            "Epoch 49, loss 0.03836691973849607\n",
            "Epoch 50, loss 0.03812062745652054\n",
            "Epoch 51, loss 0.037889742124883316\n",
            "Epoch 52, loss 0.03766730089793662\n",
            "Epoch 53, loss 0.03745511109077558\n",
            "Epoch 54, loss 0.0372494809604236\n",
            "Epoch 55, loss 0.03705252506228567\n",
            "Epoch 56, loss 0.036864660262488934\n",
            "Epoch 57, loss 0.036683205359036825\n",
            "Epoch 58, loss 0.03650978262339341\n",
            "Epoch 59, loss 0.03634288606895668\n",
            "Epoch 60, loss 0.036182682489562634\n",
            "Epoch 61, loss 0.03603214315582873\n",
            "Epoch 62, loss 0.03588644485189288\n",
            "Epoch 63, loss 0.03574475055446967\n",
            "Epoch 64, loss 0.035610129244542696\n",
            "Epoch 65, loss 0.03547883691612726\n",
            "Epoch 66, loss 0.03534242470192935\n",
            "Epoch 67, loss 0.03521185249159262\n",
            "Epoch 68, loss 0.03508693581256852\n",
            "Epoch 69, loss 0.034965531391703594\n",
            "Epoch 70, loss 0.034848733778045524\n",
            "Epoch 71, loss 0.03473649256234814\n",
            "Epoch 72, loss 0.034629873645163366\n",
            "Epoch 73, loss 0.0345270697476881\n",
            "Epoch 74, loss 0.0344280020658851\n",
            "Epoch 75, loss 0.034336391194645224\n",
            "Epoch 76, loss 0.03425217127770368\n",
            "Epoch 77, loss 0.03417080927740286\n",
            "Epoch 78, loss 0.03409386944759246\n",
            "Epoch 79, loss 0.03401876011458665\n",
            "Epoch 80, loss 0.03394634718770748\n",
            "Epoch 81, loss 0.03387420373777884\n",
            "Epoch 82, loss 0.03380405817171307\n",
            "Epoch 83, loss 0.033736214937757296\n",
            "Epoch 84, loss 0.03366845263064041\n",
            "Epoch 85, loss 0.03360313108547719\n",
            "Epoch 86, loss 0.033539679669655646\n",
            "Epoch 87, loss 0.03347792445866846\n",
            "Epoch 88, loss 0.033417476711171054\n",
            "Epoch 89, loss 0.03335662587640805\n",
            "Epoch 90, loss 0.03329721560168623\n",
            "Epoch 91, loss 0.03323742528498115\n",
            "Epoch 92, loss 0.033177864552848095\n",
            "Epoch 93, loss 0.03311978913732665\n",
            "Epoch 94, loss 0.033063249355840985\n",
            "Epoch 95, loss 0.03300795177791037\n",
            "Epoch 96, loss 0.032954044284483175\n",
            "Epoch 97, loss 0.03290235983263936\n",
            "Epoch 98, loss 0.03285328522272345\n",
            "Epoch 99, loss 0.0328060154876145\n",
            "Epoch 100, loss 0.03275944000715583\n",
            "Epoch 101, loss 0.03271385713130649\n",
            "Epoch 102, loss 0.03266923053514168\n",
            "Epoch 103, loss 0.03262600140172077\n",
            "Epoch 104, loss 0.03258381845717936\n",
            "Epoch 105, loss 0.03254264844952084\n",
            "Epoch 106, loss 0.03250306916188978\n",
            "Epoch 107, loss 0.032464166651190694\n",
            "Epoch 108, loss 0.032426332099629473\n",
            "Epoch 109, loss 0.03238938919911613\n",
            "Epoch 110, loss 0.032353193805216375\n",
            "Epoch 111, loss 0.032317521339685455\n",
            "Epoch 112, loss 0.03228208154787508\n",
            "Epoch 113, loss 0.03224693953681181\n",
            "Epoch 114, loss 0.03221233253087788\n",
            "Epoch 115, loss 0.03217859184603346\n",
            "Epoch 116, loss 0.032145242012965466\n",
            "Epoch 117, loss 0.03211238770166987\n",
            "Epoch 118, loss 0.032079468815843074\n",
            "Epoch 119, loss 0.032046872328620364\n",
            "Epoch 120, loss 0.03201432753333505\n",
            "Epoch 121, loss 0.03198251980289582\n",
            "Epoch 122, loss 0.03195085640577332\n",
            "Epoch 123, loss 0.03191959662817578\n",
            "Epoch 124, loss 0.031888743669952924\n",
            "Epoch 125, loss 0.03185827894481594\n",
            "Epoch 126, loss 0.03182447099851185\n",
            "Epoch 127, loss 0.031792132908294864\n",
            "Epoch 128, loss 0.03175926996950815\n",
            "Epoch 129, loss 0.03172696563180616\n",
            "Epoch 130, loss 0.03169476001542246\n",
            "Epoch 131, loss 0.0316606706838512\n",
            "Epoch 132, loss 0.0316277688709299\n",
            "Epoch 133, loss 0.03159565914316814\n",
            "Epoch 134, loss 0.031563898982827215\n",
            "Epoch 135, loss 0.03153248128807941\n",
            "Epoch 136, loss 0.03150169887323227\n",
            "Epoch 137, loss 0.03147152739658094\n",
            "Epoch 138, loss 0.03144184346059611\n",
            "Epoch 139, loss 0.03141203097168578\n",
            "Epoch 140, loss 0.03138144921969313\n",
            "Epoch 141, loss 0.03135114726617523\n",
            "Epoch 142, loss 0.03132138814116822\n",
            "Epoch 143, loss 0.03129197834441239\n",
            "Epoch 144, loss 0.03126288554192174\n",
            "Epoch 145, loss 0.03123429846561511\n",
            "Epoch 146, loss 0.031205923612897014\n",
            "Epoch 147, loss 0.031178096114743568\n",
            "Epoch 148, loss 0.031150479932347666\n",
            "Epoch 149, loss 0.031123116394914576\n",
            "Epoch 150, loss 0.031096388623383545\n",
            "Epoch 151, loss 0.0310698189522549\n",
            "Epoch 152, loss 0.03104364920051344\n",
            "Epoch 153, loss 0.03101778470098358\n",
            "Epoch 154, loss 0.03099210884998858\n",
            "Epoch 155, loss 0.030966910982787553\n",
            "Epoch 156, loss 0.030941806257079225\n",
            "Epoch 157, loss 0.03091772861032522\n",
            "Epoch 158, loss 0.030893938633703\n",
            "Epoch 159, loss 0.030870426115051232\n",
            "Epoch 160, loss 0.030847077142766445\n",
            "Epoch 161, loss 0.030824063925673117\n",
            "Epoch 162, loss 0.030801286375810037\n",
            "Epoch 163, loss 0.030778660056283755\n",
            "Epoch 164, loss 0.03075629337776377\n",
            "Epoch 165, loss 0.030734245289832502\n",
            "Epoch 166, loss 0.030711964482614196\n",
            "Epoch 167, loss 0.030690106125691827\n",
            "Epoch 168, loss 0.03066824210318372\n",
            "Epoch 169, loss 0.030646252246651483\n",
            "Epoch 170, loss 0.03062446028339721\n",
            "Epoch 171, loss 0.030602902496638152\n",
            "Epoch 172, loss 0.03058145463824786\n",
            "Epoch 173, loss 0.030560179025829717\n",
            "Epoch 174, loss 0.03053913062612449\n",
            "Epoch 175, loss 0.03051826198707817\n",
            "Epoch 176, loss 0.03049746644838326\n",
            "Epoch 177, loss 0.03047688405553937\n",
            "Epoch 178, loss 0.030456507283877558\n",
            "Epoch 179, loss 0.030436222547261457\n",
            "Epoch 180, loss 0.030416068534127576\n",
            "Epoch 181, loss 0.030396254531064485\n",
            "Epoch 182, loss 0.030376654999810588\n",
            "Epoch 183, loss 0.030357343854867404\n",
            "Epoch 184, loss 0.03033819323501505\n",
            "Epoch 185, loss 0.03031886611739743\n",
            "Epoch 186, loss 0.030299622904665986\n",
            "Epoch 187, loss 0.030280479879684064\n",
            "Epoch 188, loss 0.030261582454324275\n",
            "Epoch 189, loss 0.030242757459522235\n",
            "Epoch 190, loss 0.03022400184001621\n",
            "Epoch 191, loss 0.030205472357210807\n",
            "Epoch 192, loss 0.030187065133468884\n",
            "Epoch 193, loss 0.030168701265230483\n",
            "Epoch 194, loss 0.03015045634306608\n",
            "Epoch 195, loss 0.030132129133713104\n",
            "Epoch 196, loss 0.0301133880795985\n",
            "Epoch 197, loss 0.03009481435929058\n",
            "Epoch 198, loss 0.030076526811380077\n",
            "Epoch 199, loss 0.030058200891971584\n",
            "Epoch 200, loss 0.03003984212531681\n",
            "Epoch 201, loss 0.030021262988527458\n",
            "Epoch 202, loss 0.03000281710325797\n",
            "Epoch 203, loss 0.02998435323123791\n",
            "Epoch 204, loss 0.029966099648424338\n",
            "Epoch 205, loss 0.029947814373068034\n",
            "Epoch 206, loss 0.029929659711641785\n",
            "Epoch 207, loss 0.029911711235478942\n",
            "Epoch 208, loss 0.029893742649051216\n",
            "Epoch 209, loss 0.029875999130972764\n",
            "Epoch 210, loss 0.029858448100872882\n",
            "Epoch 211, loss 0.02984101256518853\n",
            "Epoch 212, loss 0.02982378164596452\n",
            "Epoch 213, loss 0.02980666365759039\n",
            "Epoch 214, loss 0.029789583683087352\n",
            "Epoch 215, loss 0.029772695224371502\n",
            "Epoch 216, loss 0.029756027032561878\n",
            "Epoch 217, loss 0.029739405853529282\n",
            "Epoch 218, loss 0.029722907090350806\n",
            "Epoch 219, loss 0.029706459117150905\n",
            "Epoch 220, loss 0.029690097592165836\n",
            "Epoch 221, loss 0.02967382836742762\n",
            "Epoch 222, loss 0.029656730172846837\n",
            "Epoch 223, loss 0.029637289835043327\n",
            "Epoch 224, loss 0.029617830402732866\n",
            "Epoch 225, loss 0.02959803101267538\n",
            "Epoch 226, loss 0.029577950313926037\n",
            "Epoch 227, loss 0.029556874110037903\n",
            "Epoch 228, loss 0.02953588016946884\n",
            "Epoch 229, loss 0.02951502496698248\n",
            "Epoch 230, loss 0.029494293807751037\n",
            "Epoch 231, loss 0.029473932495779957\n",
            "Epoch 232, loss 0.029453707619408746\n",
            "Epoch 233, loss 0.029432410271182822\n",
            "Epoch 234, loss 0.029411469628856865\n",
            "Epoch 235, loss 0.0293897501253332\n",
            "Epoch 236, loss 0.029367369515211336\n",
            "Epoch 237, loss 0.029344905212778848\n",
            "Epoch 238, loss 0.029322553837622466\n",
            "Epoch 239, loss 0.02929997357505497\n",
            "Epoch 240, loss 0.029277347804818367\n",
            "Epoch 241, loss 0.029254591614279936\n",
            "Epoch 242, loss 0.02923081282187261\n",
            "Epoch 243, loss 0.029207559822888873\n",
            "Epoch 244, loss 0.02918385756217865\n",
            "Epoch 245, loss 0.02916060883560624\n",
            "Epoch 246, loss 0.02913723084050699\n",
            "Epoch 247, loss 0.02911406951145377\n",
            "Epoch 248, loss 0.02909100709522703\n",
            "Epoch 249, loss 0.029067919186346228\n",
            "Epoch 250, loss 0.029044393824628933\n",
            "Epoch 251, loss 0.029019120663018816\n",
            "Epoch 252, loss 0.02899501229224244\n",
            "Epoch 253, loss 0.028970805364906562\n",
            "Epoch 254, loss 0.0289463447140725\n",
            "Epoch 255, loss 0.02892189626016566\n",
            "Epoch 256, loss 0.028897320727256225\n",
            "Epoch 257, loss 0.028873696223979377\n",
            "Epoch 258, loss 0.02884979806723207\n",
            "Epoch 259, loss 0.028825954762282332\n",
            "Epoch 260, loss 0.028802683766766295\n",
            "Epoch 261, loss 0.02877870803676386\n",
            "Epoch 262, loss 0.028754414296205774\n",
            "Epoch 263, loss 0.02873045157471881\n",
            "Epoch 264, loss 0.028706224676052185\n",
            "Epoch 265, loss 0.028682367385047158\n",
            "Epoch 266, loss 0.02865846515644332\n",
            "Epoch 267, loss 0.028634430761546117\n",
            "Epoch 268, loss 0.02861091641710745\n",
            "Epoch 269, loss 0.028588437294751433\n",
            "Epoch 270, loss 0.02856565217889452\n",
            "Epoch 271, loss 0.028542934951021105\n",
            "Epoch 272, loss 0.028520514071439705\n",
            "Epoch 273, loss 0.028499384082722373\n",
            "Epoch 274, loss 0.02847798780794957\n",
            "Epoch 275, loss 0.02845672341220383\n",
            "Epoch 276, loss 0.028435788250505682\n",
            "Epoch 277, loss 0.02841475861104852\n",
            "Epoch 278, loss 0.028393637987557882\n",
            "Epoch 279, loss 0.02837227098688179\n",
            "Epoch 280, loss 0.028351246507976033\n",
            "Epoch 281, loss 0.028329293814108487\n",
            "Epoch 282, loss 0.02830623890761825\n",
            "Epoch 283, loss 0.02828282029117248\n",
            "Epoch 284, loss 0.02825917656886391\n",
            "Epoch 285, loss 0.028235385964487946\n",
            "Epoch 286, loss 0.02821215348143642\n",
            "Epoch 287, loss 0.028189207362189855\n",
            "Epoch 288, loss 0.02816612773622419\n",
            "Epoch 289, loss 0.02814307184624901\n",
            "Epoch 290, loss 0.028120032601412614\n",
            "Epoch 291, loss 0.028097157267858268\n",
            "Epoch 292, loss 0.028074299062354143\n",
            "Epoch 293, loss 0.028051502206849425\n",
            "Epoch 294, loss 0.028028829771665695\n",
            "Epoch 295, loss 0.028006313068257408\n",
            "Epoch 296, loss 0.027983904825269315\n",
            "Epoch 297, loss 0.027961413522021968\n",
            "Epoch 298, loss 0.027939687996282103\n",
            "Epoch 299, loss 0.027915419278551328\n",
            "Epoch 300, loss 0.027891683147221825\n",
            "Epoch 301, loss 0.027868162339408694\n",
            "Epoch 302, loss 0.02784466879374232\n",
            "Epoch 303, loss 0.027821365960800933\n",
            "Epoch 304, loss 0.02779820709756118\n",
            "Epoch 305, loss 0.027775163862507196\n",
            "Epoch 306, loss 0.027752452967845995\n",
            "Epoch 307, loss 0.02772974362614289\n",
            "Epoch 308, loss 0.027706928252420954\n",
            "Epoch 309, loss 0.027683257162211157\n",
            "Epoch 310, loss 0.027659756681212202\n",
            "Epoch 311, loss 0.02763646269580815\n",
            "Epoch 312, loss 0.027613416917221762\n",
            "Epoch 313, loss 0.02759019478378342\n",
            "Epoch 314, loss 0.027566497964941518\n",
            "Epoch 315, loss 0.02754215044022457\n",
            "Epoch 316, loss 0.027517936197302414\n",
            "Epoch 317, loss 0.02749392054668928\n",
            "Epoch 318, loss 0.027469859383741056\n",
            "Epoch 319, loss 0.027445345441900554\n",
            "Epoch 320, loss 0.027420693367398323\n",
            "Epoch 321, loss 0.02739648889764286\n",
            "Epoch 322, loss 0.027372122335313286\n",
            "Epoch 323, loss 0.027347762299887283\n",
            "Epoch 324, loss 0.02732316004271231\n",
            "Epoch 325, loss 0.02729855719390563\n",
            "Epoch 326, loss 0.02727390981200798\n",
            "Epoch 327, loss 0.027249559003191627\n",
            "Epoch 328, loss 0.027225023170078452\n",
            "Epoch 329, loss 0.027201057203903808\n",
            "Epoch 330, loss 0.027176745565328186\n",
            "Epoch 331, loss 0.027152723588993136\n",
            "Epoch 332, loss 0.027128764691776316\n",
            "Epoch 333, loss 0.027104709989322035\n",
            "Epoch 334, loss 0.02708090100802936\n",
            "Epoch 335, loss 0.027057048036299217\n",
            "Epoch 336, loss 0.027033695064803673\n",
            "Epoch 337, loss 0.027009910537919277\n",
            "Epoch 338, loss 0.026986539157092176\n",
            "Epoch 339, loss 0.0269630045912823\n",
            "Epoch 340, loss 0.026939589271431492\n",
            "Epoch 341, loss 0.02691590604587643\n",
            "Epoch 342, loss 0.02689201785641631\n",
            "Epoch 343, loss 0.026868540775204047\n",
            "Epoch 344, loss 0.02684474111532759\n",
            "Epoch 345, loss 0.026821341180973773\n",
            "Epoch 346, loss 0.026797693638448992\n",
            "Epoch 347, loss 0.0267744006124854\n",
            "Epoch 348, loss 0.026750927803986076\n",
            "Epoch 349, loss 0.026726642032286422\n",
            "Epoch 350, loss 0.02670236644445275\n",
            "Epoch 351, loss 0.02667765187241099\n",
            "Epoch 352, loss 0.02665388740255139\n",
            "Epoch 353, loss 0.02662976872366694\n",
            "Epoch 354, loss 0.026606127731183607\n",
            "Epoch 355, loss 0.02658265355550312\n",
            "Epoch 356, loss 0.02655951797241781\n",
            "Epoch 357, loss 0.026536709088533908\n",
            "Epoch 358, loss 0.026513605140075818\n",
            "Epoch 359, loss 0.026490698683413784\n",
            "Epoch 360, loss 0.026468051373814125\n",
            "Epoch 361, loss 0.026445341374318324\n",
            "Epoch 362, loss 0.026422587684931303\n",
            "Epoch 363, loss 0.026399873179119727\n",
            "Epoch 364, loss 0.026376160812908386\n",
            "Epoch 365, loss 0.026352369534980973\n",
            "Epoch 366, loss 0.026328538799447233\n",
            "Epoch 367, loss 0.026304636924953842\n",
            "Epoch 368, loss 0.02628018462576193\n",
            "Epoch 369, loss 0.026256320782841715\n",
            "Epoch 370, loss 0.026233006423807124\n",
            "Epoch 371, loss 0.02621000940626502\n",
            "Epoch 372, loss 0.026186968917875114\n",
            "Epoch 373, loss 0.02616376517302965\n",
            "Epoch 374, loss 0.02614065227137853\n",
            "Epoch 375, loss 0.026117556876929193\n",
            "Epoch 376, loss 0.026092885374148507\n",
            "Epoch 377, loss 0.026067864677600784\n",
            "Epoch 378, loss 0.026042721995799517\n",
            "Epoch 379, loss 0.02601799946468491\n",
            "Epoch 380, loss 0.025993131973086295\n",
            "Epoch 381, loss 0.025965336476759974\n",
            "Epoch 382, loss 0.025937710612754964\n",
            "Epoch 383, loss 0.025910155829750058\n",
            "Epoch 384, loss 0.025879740651309522\n",
            "Epoch 385, loss 0.025849525704804054\n",
            "Epoch 386, loss 0.0258196566178054\n",
            "Epoch 387, loss 0.02579157953381368\n",
            "Epoch 388, loss 0.025763627967938536\n",
            "Epoch 389, loss 0.02573557302591377\n",
            "Epoch 390, loss 0.025707562777619784\n",
            "Epoch 391, loss 0.02567969272387099\n",
            "Epoch 392, loss 0.025651991144729873\n",
            "Epoch 393, loss 0.025624407261567637\n",
            "Epoch 394, loss 0.025596721647738692\n",
            "Epoch 395, loss 0.02556875324159629\n",
            "Epoch 396, loss 0.025541242502125732\n",
            "Epoch 397, loss 0.02551382050001224\n",
            "Epoch 398, loss 0.025486250721248948\n",
            "Epoch 399, loss 0.025457943659064335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzKMluHylyT5"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "view = TSNE(n_components=2, random_state=123).fit_transform(y_pred)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "PGqfHnalmDqn",
        "outputId": "84ec5bf6-e256-45ed-af0f-e220c87c227f"
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plt.scatter(view[:,0], view[:,1], c=labels, alpha=0.5)\n",
        "plt.xlabel('t-SNE-1')\n",
        "plt.ylabel('t-SNE-2')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-2a2d8542f222>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't-SNE-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't-SNE-2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x720 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}